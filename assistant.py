# assistant.py
import os
import io
import json
import base64
import logging
from dotenv import load_dotenv
from fastapi import FastAPI, HTTPException, UploadFile, File, Form
from fastapi.middleware.cors import CORSMiddleware
import azure.cognitiveservices.speech as speechsdk
from openai import AzureOpenAI
from typing import List, Dict
from pydub import AudioSegment
from pydantic import BaseModel, Field

# --- 1. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è ---
# –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ - —ç—Ç–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç –¥–ª—è –ø—Ä–æ–¥–∞–∫—à–µ–Ω–∞. –û–Ω–æ –≥–∏–±—á–µ, —á–µ–º print().
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

# --- 2. –ó–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –∏ –∫–æ–Ω—Å—Ç–∞–Ω—Ç—ã ---
load_dotenv()

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è Azure Speech
SPEECH_KEY = os.getenv("SPEECH_KEY")
SPEECH_REGION = os.getenv("SPEECH_REGION")
SPEECH_VOICE_NAME = "kk-KZ-DauletNeural" # –ì–æ–ª–æ—Å –¥–ª—è —Å–∏–Ω—Ç–µ–∑–∞
SPEECH_RECOGNITION_LANGUAGE = "kk-KZ"   # –Ø–∑—ã–∫ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è Azure OpenAI
AZURE_OPENAI_KEY = os.getenv("AZURE_OPENAI_KEY")
AZURE_OPENAI_ENDPOINT = os.getenv("AZURE_OPENAI_ENDPOINT")
OPENAI_API_VERSION = os.getenv("OPENAI_API_VERSION")
AZURE_OPENAI_DEPLOYMENT_NAME = os.getenv("AZURE_OPENAI_DEPLOYMENT_NAME")

# –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞
SYSTEM_PROMPT = "–°–µ–Ω ‚Äì —Ç–∞—Ä–∏—Ö –ø”ô–Ω—ñ–Ω—ñ“£ —Å–∞—Ä–∞–ø—à—ã—Å—ã, –ë–∞—Ç—ã—Ä –∞—Ç—Ç—ã AI-–∫”©–º–µ–∫—à—ñ—Å—ñ“£. “ö—ã—Å“õ–∞, “õ“±—Ä–º–µ—Ç–ø–µ–Ω –∂”ô–Ω–µ –º”ô–Ω—ñ –±–æ–π—ã–Ω—à–∞ –∂–∞—É–∞–ø –±–µ—Ä. –°–µ–Ω—ñ“£ –º—ñ–Ω–¥–µ—Ç—ñ“£ ‚Äì –±—ñ–ª—ñ–º –±–µ—Ä—É. –ü–∞–π–¥–∞–ª–∞–Ω—É—à—ã–º–µ–Ω —Å“±—Ö–±–∞—Ç –∂“Ø—Ä–≥—ñ–∑."

# --- 3. –ü—Ä–æ–≤–µ—Ä–∫–∏ –∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∏–µ–Ω—Ç–æ–≤ ---
if not all([SPEECH_KEY, SPEECH_REGION, AZURE_OPENAI_KEY, AZURE_OPENAI_ENDPOINT, OPENAI_API_VERSION, AZURE_OPENAI_DEPLOYMENT_NAME]):
    raise RuntimeError("–û–¥–Ω–∞ –∏–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è –Ω–µ –∑–∞–¥–∞–Ω—ã. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ .env —Ñ–∞–π–ª.")

try:
    AZURE_OPENAI_CLIENT = AzureOpenAI(
        api_key=AZURE_OPENAI_KEY,
        api_version=OPENAI_API_VERSION,
        azure_endpoint=AZURE_OPENAI_ENDPOINT
    )
    logging.info("–ö–ª–∏–µ–Ω—Ç Azure OpenAI —É—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω.")
except Exception as e:
    logging.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∫–ª–∏–µ–Ω—Ç Azure OpenAI: {e}")
    raise

# --- 4. Pydantic-–º–æ–¥–µ–ª–∏ –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ ---
class AssistantResponse(BaseModel):
    userText: str = Field(..., description="–†–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.")
    assistantText: str = Field(..., description="–¢–µ–∫—Å—Ç–æ–≤—ã–π –æ—Ç–≤–µ—Ç –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞.")
    audioBase64: str = Field(..., description="–ê—É–¥–∏–æ–æ—Ç–≤–µ—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ Base64.")


# --- 5. –ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ FastAPI ---
app = FastAPI(
    title="Batyr AI Assistant API",
    description="–û—Ç–¥–µ–ª—å–Ω—ã–π —Å–µ—Ä–≤–∏—Å –¥–ª—è –≥–æ–ª–æ—Å–æ–≤–æ–≥–æ AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞.",
    version="1.0.0"
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # –í –ø—Ä–æ–¥–∞–∫—à–µ–Ω–µ –ª—É—á—à–µ —É–∫–∞–∑–∞—Ç—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –¥–æ–º–µ–Ω —Ñ—Ä–æ–Ω—Ç–µ–Ω–¥–∞
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# --- 6. –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º ---

def recognize_speech_from_bytes(audio_bytes: bytes) -> str:
    """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç –∞—É–¥–∏–æ-–±–∞–π—Ç—ã –∏ —Ä–∞—Å–ø–æ–∑–Ω–∞–µ—Ç —Ä–µ—á—å —Å –ø–æ–º–æ—â—å—é Azure Speech."""
    logging.info(f"–ù–∞—á–∞–ª–æ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ä–µ—á–∏. –ü–æ–ª—É—á–µ–Ω–æ –±–∞–π—Ç–æ–≤: {len(audio_bytes)}")
    
    if len(audio_bytes) < 1000: # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø–æ—á—Ç–∏ –ø—É—Å—Ç–æ–π —Ñ–∞–π–ª
        raise ValueError("–ê—É–¥–∏–æ—Ñ–∞–π–ª —Å–ª–∏—à–∫–æ–º –º–∞–ª –∏–ª–∏ –ø—É—Å—Ç.")
        
    try:
        # –í–∞–∂–Ω–æ: –¥–ª—è —ç—Ç–æ–π —á–∞—Å—Ç–∏ –Ω–∞ —Å–µ—Ä–≤–µ—Ä–µ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω FFmpeg!
        audio_segment = AudioSegment.from_file(io.BytesIO(audio_bytes))
        logging.info("–ê—É–¥–∏–æ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ –≤ pydub –¥–ª—è –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏.")
        
        audio_segment = audio_segment.set_channels(1).set_frame_rate(16000)
        
        wav_buffer = io.BytesIO()
        audio_segment.export(wav_buffer, format="wav")
        wav_bytes = wav_buffer.getvalue()
        logging.info("–ê—É–¥–∏–æ —É—Å–ø–µ—à–Ω–æ —Å–∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–æ –≤ WAV 16kHz mono.")

    except Exception as e:
        logging.error(f"üî• –û—à–∏–±–∫–∞ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ –∞—É–¥–∏–æ —Å –ø–æ–º–æ—â—å—é pydub: {e}", exc_info=True)
        raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –∞—É–¥–∏–æ—Ñ–∞–π–ª. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ FFmpeg —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –Ω–∞ —Å–µ—Ä–≤–µ—Ä–µ.")

    speech_config = speechsdk.SpeechConfig(subscription=SPEECH_KEY, region=SPEECH_REGION, speech_recognition_language=SPEECH_RECOGNITION_LANGUAGE)
    stream = speechsdk.audio.PushAudioInputStream()
    audio_config = speechsdk.audio.AudioConfig(stream=stream)
    recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config, audio_config=audio_config)
    
    stream.write(wav_bytes)
    stream.close()

    result = recognizer.recognize_once_async().get()

    if result.reason == speechsdk.ResultReason.RecognizedSpeech:
        if not result.text or result.text.isspace():
            logging.warning("–†–∞—Å–ø–æ–∑–Ω–∞–Ω –ø—É—Å—Ç–æ–π —Ç–µ–∫—Å—Ç.")
            raise ValueError("–†–∞—Å–ø–æ–∑–Ω–∞–Ω –ø—É—Å—Ç–æ–π —Ç–µ–∫—Å—Ç.")
        logging.info(f"–†–∞—Å–ø–æ–∑–Ω–∞–Ω–æ: '{result.text}'")
        return result.text
    elif result.reason == speechsdk.ResultReason.NoMatch:
        logging.warning("–†–µ—á—å –Ω–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–∞ (NoMatch).")
        raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–æ–∑–Ω–∞—Ç—å —Ä–µ—á—å.")
    elif result.reason == speechsdk.ResultReason.Canceled:
        cancellation_details = result.cancellation_details
        logging.error(f"–û—à–∏–±–∫–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è (Canceled): {cancellation_details.reason}. –î–µ—Ç–∞–ª–∏: {cancellation_details.error_details}")
        raise RuntimeError(f"–û—à–∏–±–∫–∞ —Å–µ—Ä–≤–∏—Å–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è: {cancellation_details.reason}")
    
    raise RuntimeError("–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–∏ —Ä–µ—á–∏.")


def get_answer_from_llm(question: str, history: List[Dict[str, str]]) -> str:
    """–ü–æ–ª—É—á–∞–µ—Ç –æ—Ç–≤–µ—Ç –æ—Ç —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏ Azure OpenAI."""
    messages = [{"role": "system", "content": SYSTEM_PROMPT}] + history + [{"role": "user", "content": question}]
    logging.info(f"–û—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –≤ Azure OpenAI —Å {len(messages)} —Å–æ–æ–±—â–µ–Ω–∏—è–º–∏.")
    
    try:
        response = AZURE_OPENAI_CLIENT.chat.completions.create(
            model=AZURE_OPENAI_DEPLOYMENT_NAME,
            messages=messages,
            temperature=0.7,
            max_tokens=150
        )
        answer = response.choices[0].message.content
        logging.info(f"–û—Ç–≤–µ—Ç –æ—Ç LLM –ø–æ–ª—É—á–µ–Ω: '{answer[:50]}...'")
        return answer
    except Exception as e:
        logging.error(f"üî• –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞—â–µ–Ω–∏–∏ –∫ Azure OpenAI: {e}", exc_info=True)
        # –ü—Ä–æ–±—Ä–∞—Å—ã–≤–∞–µ–º –æ—à–∏–±–∫—É –≤—ã—à–µ, —á—Ç–æ–±—ã —ç–Ω–¥–ø–æ–∏–Ω—Ç –≤–µ—Ä–Ω—É–ª –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π —Å—Ç–∞—Ç—É—Å 500
        raise RuntimeError("–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞—â–µ–Ω–∏–∏ –∫ —Å–µ—Ä–≤–∏—Å—É OpenAI.")


def synthesize_speech_from_text(text: str) -> bytes:
    """–°–∏–Ω—Ç–µ–∑–∏—Ä—É–µ—Ç —Ä–µ—á—å –∏–∑ —Ç–µ–∫—Å—Ç–∞ —Å –ø–æ–º–æ—â—å—é Azure Speech."""
    logging.info(f"–ù–∞—á–∞–ª–æ —Å–∏–Ω—Ç–µ–∑–∞ —Ä–µ—á–∏ –¥–ª—è —Ç–µ–∫—Å—Ç–∞: '{text[:50]}...'")
    speech_config = speechsdk.SpeechConfig(subscription=SPEECH_KEY, region=SPEECH_REGION)
    speech_config.speech_synthesis_voice_name = SPEECH_VOICE_NAME
    speech_config.set_speech_synthesis_output_format(speechsdk.SpeechSynthesisOutputFormat.Audio16Khz32KBitRateMonoMp3)
    
    synthesizer = speechsdk.SpeechSynthesizer(speech_config=speech_config, audio_config=None)
    result = synthesizer.speak_text_async(text).get()
    
    if result.reason == speechsdk.ResultReason.SynthesizingAudioCompleted:
        logging.info(f"–°–∏–Ω—Ç–µ–∑ —Ä–µ—á–∏ —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω. –†–∞–∑–º–µ—Ä –∞—É–¥–∏–æ: {len(result.audio_data)} –±–∞–π—Ç.")
        return result.audio_data
    
    cancellation_details = result.cancellation_details
    logging.error(f"–û—à–∏–±–∫–∞ —Å–∏–Ω—Ç–µ–∑–∞ —Ä–µ—á–∏: {cancellation_details.reason}. –î–µ—Ç–∞–ª–∏: {cancellation_details.error_details}")
    raise RuntimeError(f"–û—à–∏–±–∫–∞ —Å–µ—Ä–≤–∏—Å–∞ —Å–∏–Ω—Ç–µ–∑–∞ —Ä–µ—á–∏: {cancellation_details.reason}")


# --- 7. –§–∏–Ω–∞–ª—å–Ω—ã–π —ç–Ω–¥–ø–æ–∏–Ω—Ç —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫ ---
@app.post("/api/ask-assistant", response_model=AssistantResponse)
async def ask_assistant(
    audio_file: UploadFile = File(...),
    history_json: str = Form("[]")
):
    try:
        try:
            history = json.loads(history_json)
            if not isinstance(history, list):
                history = []
        except json.JSONDecodeError:
            raise HTTPException(status_code=400, detail="–ù–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç JSON –≤ –ø–æ–ª–µ history_json.")

        audio_bytes = await audio_file.read()
        
        # –®–∞–≥ 1: –†–∞—Å–ø–æ–∑–Ω–∞—Ç—å —Ä–µ—á—å
        recognized_text = recognize_speech_from_bytes(audio_bytes)
        
        # –®–∞–≥ 2: –ü–æ–ª—É—á–∏—Ç—å –æ—Ç–≤–µ—Ç –æ—Ç LLM
        answer_text = get_answer_from_llm(recognized_text, history)
        
        # –®–∞–≥ 3: –°–∏–Ω—Ç–µ–∑–∏—Ä–æ–≤–∞—Ç—å –∞—É–¥–∏–æ–æ—Ç–≤–µ—Ç
        answer_audio_bytes = synthesize_speech_from_text(answer_text)

        # –®–∞–≥ 4: –ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –æ—Ç–≤–µ—Ç
        audio_base64 = base64.b64encode(answer_audio_bytes).decode('utf-8')

        return AssistantResponse(
            userText=recognized_text,
            assistantText=answer_text,
            audioBase64=audio_base64
        )

    except ValueError as e:
        # –û—à–∏–±–∫–∏, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å –Ω–µ–≤–µ—Ä–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ –æ—Ç –∫–ª–∏–µ–Ω—Ç–∞ (–ø—É—Å—Ç–æ–µ –∞—É–¥–∏–æ, –Ω–µ—Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω–∞—è —Ä–µ—á—å)
        logging.warning(f"–û—à–∏–±–∫–∞ –¥–∞–Ω–Ω—ã—Ö –æ—Ç –∫–ª–∏–µ–Ω—Ç–∞ (400): {e}")
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        # –í—Å–µ –æ—Å—Ç–∞–ª—å–Ω—ã–µ, –Ω–µ–ø—Ä–µ–¥–≤–∏–¥–µ–Ω–Ω—ã–µ –æ—à–∏–±–∫–∏
        logging.error("–ü—Ä–æ–∏–∑–æ—à–ª–∞ –Ω–µ–ø—Ä–µ–¥–≤–∏–¥–µ–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –≤ —ç–Ω–¥–ø–æ–∏–Ω—Ç–µ /api/ask-assistant", exc_info=True)
        raise HTTPException(status_code=500, detail="–ü—Ä–æ–∏–∑–æ—à–ª–∞ –Ω–µ–ø—Ä–µ–¥–≤–∏–¥–µ–Ω–Ω–∞—è –≤–Ω—É—Ç—Ä–µ–Ω–Ω—è—è –æ—à–∏–±–∫–∞ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞.")