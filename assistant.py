# assistant.py
import os
import io
import traceback
from dotenv import load_dotenv
from fastapi import FastAPI, HTTPException, UploadFile, File
from fastapi.responses import StreamingResponse
from fastapi.middleware.cors import CORSMiddleware
import azure.cognitiveservices.speech as speechsdk
from openai import AzureOpenAI

# –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
load_dotenv()

# --- –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è Azure ---
SPEECH_KEY = os.getenv("SPEECH_KEY")
SPEECH_REGION = os.getenv("SPEECH_REGION")
AZURE_OPENAI_DEPLOYMENT_NAME = os.getenv("AZURE_OPENAI_DEPLOYMENT_NAME")

# --- –ü—Ä–æ–≤–µ—Ä–∫–∏ –∏ –∫–ª–∏–µ–Ω—Ç—ã ---
if not (SPEECH_KEY and SPEECH_REGION):
    raise RuntimeError("–ù–µ –Ω–∞–π–¥–µ–Ω—ã SPEECH_KEY –∏–ª–∏ SPEECH_REGION –≤ .env —Ñ–∞–π–ª–µ")
if not os.getenv("AZURE_OPENAI_KEY"):
    raise RuntimeError("–ù–µ –Ω–∞–π–¥–µ–Ω AZURE_OPENAI_KEY –≤ .env —Ñ–∞–π–ª–µ")

AZURE_OPENAI_CLIENT = AzureOpenAI(
    api_key=os.getenv("AZURE_OPENAI_KEY"),
    api_version=os.getenv("OPENAI_API_VERSION"),
    azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT")
)

# --- –ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ FastAPI –¥–ª—è –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞ ---
app = FastAPI(
    title="Batyr AI Assistant API",
    description="–û—Ç–¥–µ–ª—å–Ω—ã–π —Å–µ—Ä–≤–∏—Å –¥–ª—è –≥–æ–ª–æ—Å–æ–≤–æ–≥–æ AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞."
)

# –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º CORS, —á—Ç–æ–±—ã —Ñ—Ä–æ–Ω—Ç–µ–Ω–¥ –º–æ–≥ –æ–±—Ä–∞—â–∞—Ç—å—Å—è –∫ —ç—Ç–æ–º—É —Å–µ—Ä–≤–∏—Å—É
origins = [
    "http://localhost:3000",
    "https://batyrai.com",
    "https://www.batyrai.com",
    "https://batyr-ai.vercel.app",
    "https://batyr-ai-madis-projects-f57aa02c.vercel.app"
]
app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ ---
def recognize_speech_from_bytes(audio_bytes: bytes) -> str:
    speech_config = speechsdk.SpeechConfig(
        subscription=SPEECH_KEY, 
        region=SPEECH_REGION,
        speech_recognition_language="kk-KZ"
    )

    # ‚úÖ –ò–ó–ú–ï–ù–ï–ù–ò–ï –ó–î–ï–°–¨
    # –°–æ–∑–¥–∞–µ–º AudioConfig, –∫–æ—Ç–æ—Ä—ã–π —Ä–∞–±–æ—Ç–∞–µ—Ç —Å –ø–æ—Ç–æ–∫–æ–º –±–∞–π—Ç–æ–≤ –≤ –ø–∞–º—è—Ç–∏.
    # –≠—Ç–æ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π —Å–ø–æ—Å–æ–± –¥–ª—è Azure Speech SDK.
    stream = speechsdk.audio.PushAudioInputStream()
    audio_config = speechsdk.audio.AudioConfig(stream=stream)
    
    # –°–æ–∑–¥–∞–µ–º —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞—Ç–µ–ª—å —Å –Ω–æ–≤–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π
    recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config, audio_config=audio_config)
    
    # "–°–∫–∞—Ä–º–ª–∏–≤–∞–µ–º" –Ω–∞—à–∏ –±–∞–π—Ç—ã –≤ –ø–æ—Ç–æ–∫
    stream.write(audio_bytes)
    stream.close() # –í–∞–∂–Ω–æ –∑–∞–∫—Ä—ã—Ç—å –ø–æ—Ç–æ–∫, —á—Ç–æ–±—ã SDK –∑–Ω–∞–ª–æ, —á—Ç–æ –∞—É–¥–∏–æ –∑–∞–∫–æ–Ω—á–∏–ª–æ—Å—å

    # –†–∞—Å–ø–æ–∑–Ω–∞–µ–º
    result = recognizer.recognize_once_async().get()

    if result.reason == speechsdk.ResultReason.RecognizedSpeech:
        print(f"–†–∞—Å–ø–æ–∑–Ω–∞–Ω–æ: '{result.text}'")
        return result.text
    elif result.reason == speechsdk.ResultReason.NoMatch:
        # –≠—Ç–æ—Ç —Å–ª—É—á–∞–π —Ç–µ–ø–µ—Ä—å –º–µ–Ω–µ–µ –≤–µ—Ä–æ—è—Ç–µ–Ω, –Ω–æ –æ—Å—Ç–∞–≤–∏–º –¥–ª—è –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏
        print("–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–æ–∑–Ω–∞—Ç—å —Ä–µ—á—å.")
        raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–æ–∑–Ω–∞—Ç—å —Ä–µ—á—å. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —Å–Ω–æ–≤–∞.")
    elif result.reason == speechsdk.ResultReason.Canceled:
        cancellation_details = result.cancellation_details
        print(f"–†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –æ—Ç–º–µ–Ω–µ–Ω–æ: {cancellation_details.reason}")
        if cancellation_details.reason == speechsdk.CancellationReason.Error:
            print(f"–ö–æ–¥ –æ—à–∏–±–∫–∏: {cancellation_details.error_details}")
        raise RuntimeError(f"–û—à–∏–±–∫–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è: {cancellation_details.reason}")
    
    # –î–æ–±–∞–≤–∏–º –æ–±—Ä–∞–±–æ—Ç–∫—É –¥—Ä—É–≥–∏—Ö –≤–æ–∑–º–æ–∂–Ω—ã—Ö —Å—Ç–∞—Ç—É—Å–æ–≤
    raise RuntimeError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è: {result.reason}")


def get_answer_from_llm(question: str) -> str:
    # ... (–∫–æ–¥ —ç—Ç–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ –æ—Å—Ç–∞–µ—Ç—Å—è –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π) ...
    system_prompt = "–¢—ã ‚Äî AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç, —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∏—Å—Ç–æ—Ä–∏–∏ –ö–∞–∑–∞—Ö—Å—Ç–∞–Ω–∞. –û—Ç–≤–µ—á–∞–π –∫—Ä–∞—Ç–∫–æ, —É–≤–∞–∂–∏—Ç–µ–ª—å–Ω–æ –∏ –ø–æ-—Å—É—â–µ—Å—Ç–≤—É. –¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî –ø—Ä–æ—Å–≤–µ—â–∞—Ç—å."
    try:
        response = AZURE_OPENAI_CLIENT.chat.completions.create(
            model=AZURE_OPENAI_DEPLOYMENT_NAME,
            messages=[ {"role": "system", "content": system_prompt}, {"role": "user", "content": question} ],
            temperature=0.7, max_tokens=150,
        )
        answer = response.choices[0].message.content
        print(f"–û—Ç–≤–µ—Ç –æ—Ç LLM: '{answer}'")
        return answer
    except Exception as e:
        print(f"üî• –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞—â–µ–Ω–∏–∏ –∫ Azure OpenAI: {e}")
        return "–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, —É –º–µ–Ω—è –≤–æ–∑–Ω–∏–∫–ª–∞ –≤–Ω—É—Ç—Ä–µ–Ω–Ω—è—è –æ—à–∏–±–∫–∞. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —Å–ø—Ä–æ—Å–∏—Ç—å –ø–æ–∑–∂–µ."


def synthesize_speech_from_text(text: str) -> bytes:
    # ... (–∫–æ–¥ —ç—Ç–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ –æ—Å—Ç–∞–µ—Ç—Å—è –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π) ...
    speech_config = speechsdk.SpeechConfig(subscription=SPEECH_KEY, region=SPEECH_REGION)
    speech_config.speech_synthesis_voice_name = "kk-KZ-AigulNeural"
    synthesizer = speechsdk.SpeechSynthesizer(speech_config=speech_config, audio_config=None)
    result = synthesizer.speak_text_async(text).get()
    if result.reason == speechsdk.ResultReason.SynthesizingAudioCompleted:
        return result.audio_data
    raise RuntimeError(f"–û—à–∏–±–∫–∞ —Å–∏–Ω—Ç–µ–∑–∞ —Ä–µ—á–∏: {result.cancellation_details.reason}")


# --- –ï–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω—ã–π —ç–Ω–¥–ø–æ–∏–Ω—Ç —ç—Ç–æ–≥–æ —Å–µ—Ä–≤–∏—Å–∞ ---
@app.post("/api/ask-assistant")
async def ask_assistant(audio_file: UploadFile = File(...)):
    try:
        audio_bytes = await audio_file.read()
        recognized_text = recognize_speech_from_bytes(audio_bytes)
        answer_text = get_answer_from_llm(recognized_text)
        answer_audio_bytes = synthesize_speech_from_text(answer_text)
        return StreamingResponse(io.BytesIO(answer_audio_bytes), media_type="audio/mpeg")
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        traceback.print_exc()
        raise HTTPException(status_code=500, detail="–ü—Ä–æ–∏–∑–æ—à–ª–∞ –Ω–µ–ø—Ä–µ–¥–≤–∏–¥–µ–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞.")