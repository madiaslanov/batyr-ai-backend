# assistant.py
import os
import io
import json
import base64
import traceback
import uuid  # <-- –ò–º–ø–æ—Ä—Ç –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∏–º–µ–Ω —Ñ–∞–π–ª–æ–≤
from dotenv import load_dotenv
from fastapi import FastAPI, HTTPException, UploadFile, File, Form
from fastapi.responses import JSONResponse
from fastapi.middleware.cors import CORSMiddleware
import azure.cognitiveservices.speech as speechsdk
from openai import AzureOpenAI
from typing import List
from pydub import AudioSegment

# –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
load_dotenv()

# --- –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è Azure ---
SPEECH_KEY = os.getenv("SPEECH_KEY")
SPEECH_REGION = os.getenv("SPEECH_REGION")
AZURE_OPENAI_DEPLOYMENT_NAME = os.getenv("AZURE_OPENAI_DEPLOYMENT_NAME")

# --- –ü—Ä–æ–≤–µ—Ä–∫–∏ –∏ –∫–ª–∏–µ–Ω—Ç—ã ---
if not (SPEECH_KEY and SPEECH_REGION):
    raise RuntimeError("–ù–µ –Ω–∞–π–¥–µ–Ω—ã SPEECH_KEY –∏–ª–∏ SPEECH_REGION –≤ .env —Ñ–∞–π–ª–µ")
if not os.getenv("AZURE_OPENAI_KEY"):
    raise RuntimeError("–ù–µ –Ω–∞–π–¥–µ–Ω AZURE_OPENAI_KEY –≤ .env —Ñ–∞–π–ª–µ")

AZURE_OPENAI_CLIENT = AzureOpenAI(
    api_key=os.getenv("AZURE_OPENAI_KEY"),
    api_version=os.getenv("OPENAI_API_VERSION"),
    azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT")
)

# --- –ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ FastAPI –¥–ª—è –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞ ---
app = FastAPI(
    title="Batyr AI Assistant API",
    description="–û—Ç–¥–µ–ª—å–Ω—ã–π —Å–µ—Ä–≤–∏—Å –¥–ª—è –≥–æ–ª–æ—Å–æ–≤–æ–≥–æ AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞."
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ ---

# ‚úÖ –§–ò–ù–ê–õ–¨–ù–ê–Ø –í–ï–†–°–ò–Ø –§–£–ù–ö–¶–ò–ò –†–ê–°–ü–û–ó–ù–ê–í–ê–ù–ò–Ø –° –ú–ê–ö–°–ò–ú–ê–õ–¨–ù–û –ù–ê–î–ï–ñ–ù–û–ô –ö–û–ù–í–ï–†–¢–ê–¶–ò–ï–ô
def recognize_speech_from_bytes(audio_bytes: bytes) -> str:
    request_id = str(uuid.uuid4())
    input_filepath = f"/app/debug_audio/{request_id}_input.webm"
    output_filepath = f"/app/debug_audio/{request_id}_converted.wav"
    
    try:
        # 1. –°–Ω–∞—á–∞–ª–∞ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ–ª—É—á–µ–Ω–Ω—ã–µ –±–∞–π—Ç—ã –≤–æ –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
        with open(input_filepath, "wb") as f:
            f.write(audio_bytes)
        print(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω –≤—Ö–æ–¥—è—â–∏–π —Ñ–∞–π–ª: {input_filepath}, —Ä–∞–∑–º–µ—Ä: {len(audio_bytes)} –±–∞–π—Ç")

        # 2. –ì–æ–≤–æ—Ä–∏–º pydub –∑–∞–≥—Ä—É–∑–∏—Ç—å –∞—É–¥–∏–æ –∏–∑ —ç—Ç–æ–≥–æ —Ñ–∞–π–ª–∞. –≠—Ç–æ —Å–∞–º—ã–π –Ω–∞–¥–µ–∂–Ω—ã–π —Å–ø–æ—Å–æ–±.
        audio_segment = AudioSegment.from_file(input_filepath)

        # 3. –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ –Ω—É–∂–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç: WAV, 16kHz, –º–æ–Ω–æ
        audio_segment = audio_segment.set_channels(1).set_frame_rate(16000)

        # 4. –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ–º —Å–∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ñ–∞–π–ª –Ω–∞ –¥–∏—Å–∫
        audio_segment.export(output_filepath, format="wav")
        
        # 5. –ß–∏—Ç–∞–µ–º –±–∞–π—Ç—ã –∏–∑ —É–∂–µ –≥–æ—Ç–æ–≤–æ–≥–æ, —á–∏—Å—Ç–æ–≥–æ WAV-—Ñ–∞–π–ª–∞
        with open(output_filepath, "rb") as f:
            wav_bytes = f.read()
        print(f"–°–∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–æ –≤ WAV: {output_filepath}, —Ä–∞–∑–º–µ—Ä: {len(wav_bytes)} –±–∞–π—Ç")

    except Exception as e:
        print(f"üî• –û—à–∏–±–∫–∞ –Ω–∞ —ç—Ç–∞–ø–µ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ –∞—É–¥–∏–æ: {e}")
        raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –∞—É–¥–∏–æ—Ñ–∞–π–ª.")
    finally:
        # 6. –£–±–∏—Ä–∞–µ–º –∑–∞ —Å–æ–±–æ–π –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã –≤ –ª—é–±–æ–º —Å–ª—É—á–∞–µ
        if os.path.exists(input_filepath): os.remove(input_filepath)
        if os.path.exists(output_filepath): os.remove(output_filepath)

    # 7. –†–∞–±–æ—Ç–∞–µ–º —Å Azure SDK, –ø–µ—Ä–µ–¥–∞–≤–∞—è –µ–º—É —á–∏—Å—Ç—ã–µ WAV-–±–∞–π—Ç—ã
    speech_config = speechsdk.SpeechConfig(subscription=SPEECH_KEY, region=SPEECH_REGION, speech_recognition_language="kk-KZ")
    stream = speechsdk.audio.PushAudioInputStream()
    audio_config = speechsdk.audio.AudioConfig(stream=stream)
    recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config, audio_config=audio_config)
    
    stream.write(wav_bytes)
    stream.close() 

    result = recognizer.recognize_once_async().get()

    if result.reason == speechsdk.ResultReason.RecognizedSpeech:
        print(f"–†–∞—Å–ø–æ–∑–Ω–∞–Ω–æ: '{result.text}'")
        if not result.text or result.text.isspace():
             raise ValueError("–†–∞—Å–ø–æ–∑–Ω–∞–Ω –ø—É—Å—Ç–æ–π —Ç–µ–∫—Å—Ç.")
        return result.text
    elif result.reason == speechsdk.ResultReason.NoMatch:
        raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–æ–∑–Ω–∞—Ç—å —Ä–µ—á—å.")
    elif result.reason == speechsdk.ResultReason.Canceled:
        cancellation_details = result.cancellation_details
        if cancellation_details.reason == speechsdk.CancellationReason.Error:
            print(f"–ö–æ–¥ –æ—à–∏–±–∫–∏: {cancellation_details.error_details}")
        raise RuntimeError(f"–û—à–∏–±–∫–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è: {cancellation_details.reason}")
    raise RuntimeError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è: {result.reason}")

def get_answer_from_llm(question: str, history: List[dict]) -> str:
    system_prompt = "–°–µ–Ω ‚Äì —Ç–∞—Ä–∏—Ö –ø”ô–Ω—ñ–Ω—ñ“£ —Å–∞—Ä–∞–ø—à—ã—Å—ã, –ë–∞—Ç—ã—Ä –∞—Ç—Ç—ã AI-–∫”©–º–µ–∫—à—ñ—Å—ñ“£. “ö—ã—Å“õ–∞, “õ“±—Ä–º–µ—Ç–ø–µ–Ω –∂”ô–Ω–µ –º”ô–Ω—ñ –±–æ–π—ã–Ω—à–∞ –∂–∞—É–∞–ø –±–µ—Ä. –°–µ–Ω—ñ“£ –º—ñ–Ω–¥–µ—Ç—ñ“£ ‚Äì –±—ñ–ª—ñ–º –±–µ—Ä—É. –ü–∞–π–¥–∞–ª–∞–Ω—É—à—ã–º–µ–Ω —Å“±—Ö–±–∞—Ç –∂“Ø—Ä–≥—ñ–∑."
    messages = [{"role": "system", "content": system_prompt}] + history + [{"role": "user", "content": question}]
    try:
        response = AZURE_OPENAI_CLIENT.chat.completions.create(model=AZURE_OPENAI_DEPLOYMENT_NAME, messages=messages, temperature=0.7, max_tokens=150)
        answer = response.choices[0].message.content
        print(f"–û—Ç–≤–µ—Ç –æ—Ç LLM: '{answer}'")
        return answer
    except Exception as e:
        print(f"üî• –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞—â–µ–Ω–∏–∏ –∫ Azure OpenAI: {e}")
        return "–ö–µ—à—ñ—Ä—ñ“£—ñ–∑, –º–µ–Ω–¥–µ —ñ—à–∫—ñ “õ–∞—Ç–µ –ø–∞–π–¥–∞ –±–æ–ª–¥—ã."

def synthesize_speech_from_text(text: str) -> bytes:
    speech_config = speechsdk.SpeechConfig(subscription=SPEECH_KEY, region=SPEECH_REGION)
    speech_config.speech_synthesis_voice_name = "kk-KZ-DauletNeural"
    speech_config.set_speech_synthesis_output_format(speechsdk.SpeechSynthesisOutputFormat.Audio16Khz32KBitRateMonoMp3)
    synthesizer = speechsdk.SpeechSynthesizer(speech_config=speech_config, audio_config=None)
    result = synthesizer.speak_text_async(text).get()
    if result.reason == speechsdk.ResultReason.SynthesizingAudioCompleted:
        return result.audio_data
    raise RuntimeError(f"–û—à–∏–±–∫–∞ —Å–∏–Ω—Ç–µ–∑–∞ —Ä–µ—á–∏: {result.cancellation_details.reason}")


# --- –§–ò–ù–ê–õ–¨–ù–´–ô, –†–ê–ë–û–ß–ò–ô –≠–ù–î–ü–û–ò–ù–¢ ---
@app.post("/api/ask-assistant")
async def ask_assistant(audio_file: UploadFile = File(...), history_json: str = Form("[]")):
    try:
        history = json.loads(history_json)
        if not isinstance(history, list):
            history = []
        audio_bytes = await audio_file.read()
        recognized_text = recognize_speech_from_bytes(audio_bytes)
        answer_text = get_answer_from_llm(recognized_text, history)
        answer_audio_bytes = synthesize_speech_from_text(answer_text)
        audio_base64 = base64.b64encode(answer_audio_bytes).decode('utf-8')
        return JSONResponse(content={"userText": recognized_text, "assistantText": answer_text, "audioBase64": audio_base64})
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        traceback.print_exc()
        raise HTTPException(status_code=500, detail="–ü—Ä–æ–∏–∑–æ—à–ª–∞ –Ω–µ–ø—Ä–µ–¥–≤–∏–¥–µ–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞.")