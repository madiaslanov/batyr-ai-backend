# assistant.py
import os
import io
import json
import base64
import logging
import datetime
from dotenv import load_dotenv
from fastapi import FastAPI, HTTPException, UploadFile, File, Form
from fastapi.middleware.cors import CORSMiddleware
import azure.cognitiveservices.speech as speechsdk
from openai import AzureOpenAI
from typing import List, Dict
from pydub import AudioSegment
from pydantic import BaseModel, Field

# --- 1. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è ---
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

# --- 2. –ó–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –∏ –∫–æ–Ω—Å—Ç–∞–Ω—Ç—ã ---
load_dotenv()

SPEECH_KEY = os.getenv("SPEECH_KEY")
SPEECH_REGION = os.getenv("SPEECH_REGION")
SPEECH_VOICE_NAME = "kk-KZ-DauletNeural"
SPEECH_RECOGNITION_LANGUAGE = "kk-KZ"

AZURE_OPENAI_KEY = os.getenv("AZURE_OPENAI_KEY")
AZURE_OPENAI_ENDPOINT = os.getenv("AZURE_OPENAI_ENDPOINT")
OPENAI_API_VERSION = os.getenv("OPENAI_API_VERSION")
AZURE_OPENAI_DEPLOYMENT_NAME = os.getenv("AZURE_OPENAI_DEPLOYMENT_NAME")

SYSTEM_PROMPT = "–°–µ–Ω ‚Äì —Ç–∞—Ä–∏—Ö –ø”ô–Ω—ñ–Ω—ñ“£ —Å–∞—Ä–∞–ø—à—ã—Å—ã, –ë–∞—Ç—ã—Ä –∞—Ç—Ç—ã AI-–∫”©–º–µ–∫—à—ñ—Å—ñ“£. “ö—ã—Å“õ–∞, “õ“±—Ä–º–µ—Ç–ø–µ–Ω –∂”ô–Ω–µ –º”ô–Ω—ñ –±–æ–π—ã–Ω—à–∞ –∂–∞—É–∞–ø –±–µ—Ä. –û—Ç–≤–µ—á–∞–π 1-2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º–∏. –°–µ–Ω—ñ“£ –º—ñ–Ω–¥–µ—Ç—ñ“£ ‚Äì –±—ñ–ª—ñ–º –±–µ—Ä—É."

# --- 3. –ü—Ä–æ–≤–µ—Ä–∫–∏ –∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∏–µ–Ω—Ç–æ–≤ ---
if not all([SPEECH_KEY, SPEECH_REGION, AZURE_OPENAI_KEY, AZURE_OPENAI_ENDPOINT, OPENAI_API_VERSION, AZURE_OPENAI_DEPLOYMENT_NAME]):
    raise RuntimeError("–û–¥–Ω–∞ –∏–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è –Ω–µ –∑–∞–¥–∞–Ω—ã. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ .env —Ñ–∞–π–ª.")

try:
    AZURE_OPENAI_CLIENT = AzureOpenAI(
        api_key=AZURE_OPENAI_KEY,
        api_version=OPENAI_API_VERSION,
        azure_endpoint=AZURE_OPENAI_ENDPOINT
    )
    logging.info("–ö–ª–∏–µ–Ω—Ç Azure OpenAI —É—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω.")
except Exception as e:
    logging.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∫–ª–∏–µ–Ω—Ç Azure OpenAI: {e}")
    raise

# --- 4. Pydantic-–º–æ–¥–µ–ª–∏ ---
class AssistantResponse(BaseModel):
    userText: str = Field(..., description="–†–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.")
    assistantText: str = Field(..., description="–¢–µ–∫—Å—Ç–æ–≤—ã–π –æ—Ç–≤–µ—Ç –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞.")
    audioBase64: str = Field(..., description="–ê—É–¥–∏–æ–æ—Ç–≤–µ—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ Base64.")

# --- 5. –ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ FastAPI —Å —É—Å–ª–æ–≤–Ω—ã–º –æ—Ç–∫–ª—é—á–µ–Ω–∏–µ–º –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ ---
# –ß–∏—Ç–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è
ENVIRONMENT = os.getenv("ENVIRONMENT", "development")

fastapi_kwargs = {
    "title": "Batyr AI Assistant API",
    "description": "–û—Ç–¥–µ–ª—å–Ω—ã–π —Å–µ—Ä–≤–∏—Å –¥–ª—è –≥–æ–ª–æ—Å–æ–≤–æ–≥–æ AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞.",
    "version": "1.0.0"
}

# –ï—Å–ª–∏ –º—ã –Ω–∞ –ø—Ä–æ–¥–∞–∫—à–µ–Ω-—Å–µ—Ä–≤–µ—Ä–µ, –æ—Ç–∫–ª—é—á–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é
if ENVIRONMENT == "production":
    fastapi_kwargs["docs_url"] = None
    fastapi_kwargs["redoc_url"] = None
    fastapi_kwargs["openapi_url"] = None
    logging.info("Assistant: –ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –∑–∞–ø—É—â–µ–Ω–æ –≤ —Ä–µ–∂–∏–º–µ 'production'. –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è API –æ—Ç–∫–ª—é—á–µ–Ω–∞.")
else:
    logging.info("Assistant: –ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –∑–∞–ø—É—â–µ–Ω–æ –≤ —Ä–µ–∂–∏–º–µ 'development'. –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è API –¥–æ—Å—Ç—É–ø–Ω–∞.")

# –°–æ–∑–¥–∞–µ–º –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ —Å –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã–º–∏ –∞—Ä–≥—É–º–µ–Ω—Ç–∞–º–∏
app = FastAPI(**fastapi_kwargs)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# --- 6. –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ ---
# (–≤—Å–µ –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –æ—Å—Ç–∞—é—Ç—Å—è –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)
def recognize_speech_from_bytes(audio_bytes: bytes, original_filename: str) -> str:
    logging.info(f"–ù–∞—á–∞–ª–æ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ä–µ—á–∏. –ü–æ–ª—É—á–µ–Ω–æ –±–∞–π—Ç–æ–≤: {len(audio_bytes)}")
    
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    temp_audio_dir = "temp_audio"
    os.makedirs(temp_audio_dir, exist_ok=True)
    
    try:
        audio_segment = AudioSegment.from_file(io.BytesIO(audio_bytes))
        audio_segment = audio_segment.set_channels(1).set_frame_rate(16000)
        
        wav_filepath = os.path.join(temp_audio_dir, f"to_azure_{timestamp}.wav")
        audio_segment.export(wav_filepath, format="wav")
        logging.info(f"–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π WAV-—Ñ–∞–π–ª —Å–æ—Ö—Ä–∞–Ω–µ–Ω –¥–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è: {wav_filepath}")

    except Exception as e:
        logging.error(f"üî• –û—à–∏–±–∫–∞ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ –∞—É–¥–∏–æ: {e}", exc_info=True)
        raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –∞—É–¥–∏–æ—Ñ–∞–π–ª.")

    try:
        speech_config = speechsdk.SpeechConfig(subscription=SPEECH_KEY, region=SPEECH_REGION, speech_recognition_language=SPEECH_RECOGNITION_LANGUAGE)
        audio_config = speechsdk.audio.AudioConfig(filename=wav_filepath)
        recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config, audio_config=audio_config)
        
        logging.info("–ù–∞—á–∞–ª–æ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –∏–∑ —Ñ–∞–π–ª–∞...")
        result = recognizer.recognize_once_async().get()
        
    finally:
        try:
            os.remove(wav_filepath)
            logging.info(f"–í—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª {wav_filepath} —É–¥–∞–ª–µ–Ω.")
        except OSError as e:
            logging.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å —É–¥–∞–ª–∏—Ç—å –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª {wav_filepath}: {e}")

    if result.reason == speechsdk.ResultReason.RecognizedSpeech:
        if not result.text or result.text.isspace():
            logging.warning("–†–∞—Å–ø–æ–∑–Ω–∞–Ω –ø—É—Å—Ç–æ–π —Ç–µ–∫—Å—Ç.")
            raise ValueError("–†–∞—Å–ø–æ–∑–Ω–∞–Ω –ø—É—Å—Ç–æ–π —Ç–µ–∫—Å—Ç.")
        logging.info(f"‚úÖ –†–∞—Å–ø–æ–∑–Ω–∞–Ω–æ: '{result.text}'")
        return result.text
    elif result.reason == speechsdk.ResultReason.NoMatch:
        logging.warning("–†–µ—á—å –Ω–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–∞ (NoMatch). –í–µ—Ä–æ—è—Ç–Ω–æ, –≤ —Ñ–∞–π–ª–µ —Ç–∏—à–∏–Ω–∞ –∏–ª–∏ —à—É–º.")
        raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–æ–∑–Ω–∞—Ç—å —Ä–µ—á—å.")
    elif result.reason == speechsdk.ResultReason.Canceled:
        cancellation_details = result.cancellation_details
        logging.error(f"–û—à–∏–±–∫–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è (Canceled): {cancellation_details.reason}. –î–µ—Ç–∞–ª–∏: {cancellation_details.error_details}")
        if cancellation_details.reason == speechsdk.CancellationReason.Error:
             if cancellation_details.error_code in (speechsdk.CancellationErrorCode.ConnectionFailure, speechsdk.CancellationErrorCode.ServiceUnavailable):
                  raise RuntimeError("–û—à–∏–±–∫–∞ —Å–µ—Ç–∏ –∏–ª–∏ —Å–µ—Ä–≤–∏—Å Azure –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω.")
             if cancellation_details.error_code == speechsdk.CancellationErrorCode.AuthenticationFailure:
                  raise RuntimeError("–û—à–∏–±–∫–∞ –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –≤–∞—à SPEECH_KEY –∏ SPEECH_REGION.")
        raise RuntimeError(f"–û—à–∏–±–∫–∞ —Å–µ—Ä–≤–∏—Å–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è: {cancellation_details.reason}")
    
    raise RuntimeError("–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–∏ —Ä–µ—á–∏.")


def get_answer_from_llm(question: str, history: List[Dict[str, str]]) -> str:
    messages = [{"role": "system", "content": SYSTEM_PROMPT}] + history + [{"role": "user", "content": question}]
    logging.info(f"–û—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –≤ Azure OpenAI —Å {len(messages)} —Å–æ–æ–±—â–µ–Ω–∏—è–º–∏.")
    
    try:
        response = AZURE_OPENAI_CLIENT.chat.completions.create(
            model=AZURE_OPENAI_DEPLOYMENT_NAME,
            messages=messages,
            temperature=0.7,
            max_tokens=80
        )
        answer = response.choices[0].message.content
        logging.info(f"–û—Ç–≤–µ—Ç –æ—Ç LLM –ø–æ–ª—É—á–µ–Ω: '{answer[:50]}...'")
        return answer
    except Exception as e:
        logging.error(f"üî• –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞—â–µ–Ω–∏–∏ –∫ Azure OpenAI: {e}", exc_info=True)
        raise RuntimeError("–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞—â–µ–Ω–∏–∏ –∫ —Å–µ—Ä–≤–∏—Å—É OpenAI.")


def synthesize_speech_from_text(text: str) -> bytes:
    logging.info(f"–ù–∞—á–∞–ª–æ —Å–∏–Ω—Ç–µ–∑–∞ —Ä–µ—á–∏ –¥–ª—è —Ç–µ–∫—Å—Ç–∞: '{text[:50]}...'")
    speech_config = speechsdk.SpeechConfig(subscription=SPEECH_KEY, region=SPEECH_REGION)
    speech_config.speech_synthesis_voice_name = SPEECH_VOICE_NAME
    speech_config.set_speech_synthesis_output_format(speechsdk.SpeechSynthesisOutputFormat.Audio16Khz32KBitRateMonoMp3)
    synthesizer = speechsdk.SpeechSynthesizer(speech_config=speech_config, audio_config=None)
    result = synthesizer.speak_text_async(text).get()
    if result.reason == speechsdk.ResultReason.SynthesizingAudioCompleted:
        logging.info(f"–°–∏–Ω—Ç–µ–∑ —Ä–µ—á–∏ —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω. –†–∞–∑–º–µ—Ä –∞—É–¥–∏–æ: {len(result.audio_data)} –±–∞–π—Ç.")
        return result.audio_data
    cancellation_details = result.cancellation_details
    logging.error(f"–û—à–∏–±–∫–∞ —Å–∏–Ω—Ç–µ–∑–∞ —Ä–µ—á–∏: {cancellation_details.reason}. –î–µ—Ç–∞–ª–∏: {cancellation_details.error_details}")
    raise RuntimeError(f"–û—à–∏–±–∫–∞ —Å–µ—Ä–≤–∏—Å–∞ —Å–∏–Ω—Ç–µ–∑–∞ —Ä–µ—á–∏: {cancellation_details.reason}")


# --- 7. –§–∏–Ω–∞–ª—å–Ω—ã–π —ç–Ω–¥–ø–æ–∏–Ω—Ç ---
@app.post("/api/ask-assistant", response_model=AssistantResponse)
async def ask_assistant(
    audio_file: UploadFile = File(...),
    history_json: str = Form("[]")
):
    try:
        try:
            history = json.loads(history_json)
            if not isinstance(history, list):
                history = []
        except json.JSONDecodeError:
            raise HTTPException(status_code=400, detail="–ù–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç JSON –≤ –ø–æ–ª–µ history_json.")

        audio_bytes = await audio_file.read()
        
        recognized_text = recognize_speech_from_bytes(audio_bytes, audio_file.filename)
        
        answer_text = get_answer_from_llm(recognized_text, history)
        answer_audio_bytes = synthesize_speech_from_text(answer_text)
        audio_base64 = base64.b64encode(answer_audio_bytes).decode('utf-8')

        return AssistantResponse(
            userText=recognized_text,
            assistantText=answer_text,
            audioBase64=audio_base64
        )
    except ValueError as e:
        logging.warning(f"–û—à–∏–±–∫–∞ –¥–∞–Ω–Ω—ã—Ö –æ—Ç –∫–ª–∏–µ–Ω—Ç–∞ (400): {e}")
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        logging.error("–ü—Ä–æ–∏–∑–æ—à–ª–∞ –Ω–µ–ø—Ä–µ–¥–≤–∏–¥–µ–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –≤ —ç–Ω–¥–ø–æ–∏–Ω—Ç–µ /api/ask-assistant", exc_info=True)
        raise HTTPException(status_code=500, detail="–ü—Ä–æ–∏–∑–æ—à–ª–∞ –Ω–µ–ø—Ä–µ–¥–≤–∏–¥–µ–Ω–Ω–∞—è –≤–Ω—É—Ç—Ä–µ–Ω–Ω—è—è –æ—à–∏–±–∫–∞ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞.")