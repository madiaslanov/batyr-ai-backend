# app.py

import os
import io
import json
import base64
import logging
import hmac
import hashlib
from urllib.parse import unquote
from datetime import datetime

from flask import Flask, jsonify, abort, request, Response
from flask_cors import CORS
from dotenv import load_dotenv
from pydub import AudioSegment
import azure.cognitiveservices.speech as speechsdk
from openai import AzureOpenAI

# --- 1. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∏ –∑–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
load_dotenv()

# –ö–ª—é—á–∏ –¥–ª—è –∫–∞—Ä—Ç—ã (TTS) –∏ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞ (STT, LLM, TTS)
SPEECH_KEY = os.getenv("SPEECH_KEY")
SPEECH_REGION = os.getenv("SPEECH_REGION")
AZURE_OPENAI_KEY = os.getenv("AZURE_OPENAI_KEY")
AZURE_OPENAI_ENDPOINT = os.getenv("AZURE_OPENAI_ENDPOINT")
OPENAI_API_VERSION = os.getenv("OPENAI_API_VERSION")
AZURE_OPENAI_DEPLOYMENT_NAME = os.getenv("AZURE_OPENAI_DEPLOYMENT_NAME")
BOT_TOKEN = os.getenv("TELEGRAM_BOT_TOKEN")

# –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã –¥–ª—è –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞
SPEECH_VOICE_NAME = "kk-KZ-DauletNeural"
SPEECH_RECOGNITION_LANGUAGE = "kk-KZ"
SYSTEM_PROMPT = "–°–µ–Ω ‚Äì —Ç–∞—Ä–∏—Ö –ø”ô–Ω—ñ–Ω—ñ“£ —Å–∞—Ä–∞–ø—à—ã—Å—ã, –ë–∞—Ç—ã—Ä –∞—Ç—Ç—ã AI-–∫”©–º–µ–∫—à—ñ—Å—ñ“£. “ö—ã—Å“õ–∞, “õ“±—Ä–º–µ—Ç–ø–µ–Ω –∂”ô–Ω–µ –º”ô–Ω—ñ –±–æ–π—ã–Ω—à–∞ –∂–∞—É–∞–ø –±–µ—Ä. –û—Ç–≤–µ—á–∞–π 1-2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º–∏. –°–µ–Ω—ñ“£ –º—ñ–Ω–¥–µ—Ç—ñ“£ ‚Äì –±—ñ–ª—ñ–º –±–µ—Ä—É."

# --- 2. –ü—Ä–æ–≤–µ—Ä–∫–∏ –∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∏–µ–Ω—Ç–æ–≤ ---
# –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –≤—Å–µ—Ö –∫–ª—é—á–µ–π. –ï—Å–ª–∏ —á–µ–≥–æ-—Ç–æ –Ω–µ —Ö–≤–∞—Ç–∞–µ—Ç, –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç —Ä–∞–±–æ—Ç–∞—Ç—å –Ω–µ –±—É–¥–µ—Ç, –Ω–æ –∫–∞—Ä—Ç–∞ - –±—É–¥–µ—Ç.
if not all([SPEECH_KEY, SPEECH_REGION, AZURE_OPENAI_KEY, AZURE_OPENAI_ENDPOINT, OPENAI_API_VERSION, AZURE_OPENAI_DEPLOYMENT_NAME, BOT_TOKEN]):
    logging.warning("–û–¥–Ω–∞ –∏–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è –¥–ª—è AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞ –Ω–µ –∑–∞–¥–∞–Ω—ã. –≠–Ω–¥–ø–æ–∏–Ω—Ç /api/ask-assistant –º–æ–∂–µ—Ç –Ω–µ —Ä–∞–±–æ—Ç–∞—Ç—å.")

try:
    AZURE_OPENAI_CLIENT = AzureOpenAI(api_key=AZURE_OPENAI_KEY, api_version=OPENAI_API_VERSION, azure_endpoint=AZURE_OPENAI_ENDPOINT)
    logging.info("–ö–ª–∏–µ–Ω—Ç Azure OpenAI —É—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω.")
except Exception as e:
    logging.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∫–ª–∏–µ–Ω—Ç Azure OpenAI: {e}")

app = Flask(__name__)
CORS(app)

# --- 3. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–∞—Ä—Ç—ã (–±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π) ---
DATA_FILE = 'batyrs_data.json'
try:
    with open(DATA_FILE, 'r', encoding='utf-8') as f:
        DB_DATA = json.load(f)
    logging.info(f"‚úÖ –î–∞–Ω–Ω—ã–µ –∏–∑ —Ñ–∞–π–ª–∞ '{DATA_FILE}' —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã.")
except Exception as e:
    logging.critical(f"‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –¥–∞–Ω–Ω—ã—Ö: {e}", exc_info=True)
    DB_DATA = {}

# --- 4. –≠–Ω–¥–ø–æ–∏–Ω—Ç—ã –¥–ª—è –∫–∞—Ä—Ç—ã (–±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π) ---
@app.route('/api/region/<string:region_id>', methods=['GET'])
def get_region_info(region_id):
    logging.info(f"üêå –ó–∞–ø—Ä–æ—Å –Ω–∞ –¥–∞–Ω–Ω—ã–µ —Ä–µ–≥–∏–æ–Ω–∞: {region_id}")
    region_data = DB_DATA.get(region_id)
    if not region_data:
        return abort(404, description=f"–†–µ–≥–∏–æ–Ω —Å ID '{region_id}' –Ω–µ –Ω–∞–π–¥–µ–Ω.")
    return jsonify(region_data)

@app.route('/api/tts', methods=['POST'])
def text_to_speech_azure():
    if not all([SPEECH_KEY, SPEECH_REGION]):
        logging.error("‚ùå [TTS] –ö–ª—é—á–∏ –∏–ª–∏ —Ä–µ–≥–∏–æ–Ω –Ω–µ –Ω–∞–π–¥–µ–Ω—ã.")
        return jsonify({"error": "Azure TTS service is not configured."}), 500
    
    data = request.get_json()
    text_to_speak = data.get('text')
    if not text_to_speak:
        return jsonify({"error": "No text provided."}), 400

    logging.info(f"üîä [TTS] –ó–∞–ø—Ä–æ—Å –Ω–∞ –æ–∑–≤—É—á–∫—É —Ç–µ–∫—Å—Ç–∞: {text_to_speak[:50]}...")
    try:
        speech_config = speechsdk.SpeechConfig(subscription=SPEECH_KEY, region=SPEECH_REGION)
        speech_config.speech_synthesis_voice_name = SPEECH_VOICE_NAME
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º MP3 –¥–ª—è –ª—É—á—à–µ–≥–æ —Å–∂–∞—Ç–∏—è
        speech_config.set_speech_synthesis_output_format(speechsdk.SpeechSynthesisOutputFormat.Audio16Khz32KBitRateMonoMp3)
        synthesizer = speechsdk.SpeechSynthesizer(speech_config=speech_config, audio_config=None)
        result = synthesizer.speak_text_async(text_to_speak).get()

        if result.reason == speechsdk.ResultReason.SynthesizingAudioCompleted:
            logging.info("‚úÖ [TTS] –ê—É–¥–∏–æ —É—Å–ø–µ—à–Ω–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ.")
            return Response(result.audio_data, mimetype='audio/mp3')
        else:
            logging.error(f"‚ùå [TTS] –û—à–∏–±–∫–∞ —Å–∏–Ω—Ç–µ–∑–∞: {result.cancellation_details}")
            return jsonify({"error": "Speech synthesis failed."}), 500
    except Exception as e:
        logging.error(f"‚ùå [TTS] –í–Ω—É—Ç—Ä–µ–Ω–Ω—è—è –æ—à–∏–±–∫–∞: {e}", exc_info=True)
        return jsonify({"error": "Internal server error during TTS."}), 500


# ‚úÖ‚Üì‚Üì‚Üì –ù–û–í–´–ô –≠–ù–î–ü–û–ò–ù–¢ –ò –õ–û–ì–ò–ö–ê –î–õ–Ø –ì–û–õ–û–°–û–í–û–ì–û –ê–°–°–ò–°–¢–ï–ù–¢–ê ‚Üì‚Üì‚Üì‚úÖ

# --- 5. –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞ ---
def recognize_speech_from_bytes(audio_bytes: bytes) -> str:
    audio_segment = AudioSegment.from_file(io.BytesIO(audio_bytes))
    audio_segment = audio_segment.set_channels(1).set_frame_rate(16000)
    wav_buffer = io.BytesIO()
    audio_segment.export(wav_buffer, format="wav")
    wav_buffer.seek(0)

    speech_config = speechsdk.SpeechConfig(subscription=SPEECH_KEY, region=SPEECH_REGION, speech_recognition_language=SPEECH_RECOGNITION_LANGUAGE)
    stream = speechsdk.audio.PullAudioInputStream(wav_buffer.read())
    audio_config = speechsdk.audio.AudioConfig(stream=stream)
    recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config, audio_config=audio_config)
    result = recognizer.recognize_once_async().get()

    if result.reason == speechsdk.ResultReason.RecognizedSpeech:
        if not result.text or result.text.isspace(): raise ValueError("–†–∞—Å–ø–æ–∑–Ω–∞–Ω –ø—É—Å—Ç–æ–π —Ç–µ–∫—Å—Ç.")
        logging.info(f"‚úÖ [Assistant] –†–∞—Å–ø–æ–∑–Ω–∞–Ω–æ: '{result.text}'")
        return result.text
    elif result.reason == speechsdk.ResultReason.NoMatch:
        raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–æ–∑–Ω–∞—Ç—å —Ä–µ—á—å.")
    cancellation_details = result.cancellation_details
    logging.error(f"–û—à–∏–±–∫–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è: {cancellation_details.reason}. –î–µ—Ç–∞–ª–∏: {cancellation_details.error_details}")
    raise RuntimeError(f"–û—à–∏–±–∫–∞ —Å–µ—Ä–≤–∏—Å–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è: {cancellation_details.reason}")

def get_answer_from_llm(question: str, history: list) -> str:
    messages = [{"role": "system", "content": SYSTEM_PROMPT}] + history + [{"role": "user", "content": question}]
    try:
        response = AZURE_OPENAI_CLIENT.chat.completions.create(model=AZURE_OPENAI_DEPLOYMENT_NAME, messages=messages, temperature=0.7, max_tokens=150)
        if not response.choices or not response.choices[0].message.content:
            logging.warning("–û—Ç–≤–µ—Ç –æ—Ç LLM –±—ã–ª –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω.")
            return "–ö–µ—à—ñ—Ä—ñ“£—ñ–∑, –º–µ–Ω—ñ“£ –∂–∞—É–∞–±—ã–º –º–∞–∑–º“±–Ω —Å–∞—è—Å–∞—Ç—ã–Ω–∞ –±–∞–π–ª–∞–Ω—ã—Å—Ç—ã –±“±“ì–∞—Ç—Ç–∞–ª–¥—ã."
        answer = response.choices[0].message.content
        logging.info(f"‚úÖ [Assistant] –û—Ç–≤–µ—Ç –æ—Ç LLM –ø–æ–ª—É—á–µ–Ω: '{answer[:50]}...'")
        return answer
    except Exception as e:
        if "content_filter" in str(e):
             logging.warning(f"–ó–∞–ø—Ä–æ—Å –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω —Ñ–∏–ª—å—Ç—Ä–æ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ: {e}")
             return "–ö–µ—à—ñ—Ä—ñ“£—ñ–∑, —Å“±—Ä–∞–Ω—ã—Å—ã“£—ã–∑ –º–∞–∑–º“±–Ω —Å–∞—è—Å–∞—Ç—ã–Ω–∞ –±–∞–π–ª–∞–Ω—ã—Å—Ç—ã ”©“£–¥–µ–ª–º–µ–¥—ñ."
        logging.error(f"üî• [Assistant] –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞—â–µ–Ω–∏–∏ –∫ OpenAI: {e}", exc_info=True)
        raise RuntimeError("–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞—â–µ–Ω–∏–∏ –∫ —Å–µ—Ä–≤–∏—Å—É OpenAI.")

def synthesize_speech_for_assistant(text: str) -> bytes:
    speech_config = speechsdk.SpeechConfig(subscription=SPEECH_KEY, region=SPEECH_REGION)
    speech_config.speech_synthesis_voice_name = SPEECH_VOICE_NAME
    speech_config.set_speech_synthesis_output_format(speechsdk.SpeechSynthesisOutputFormat.Audio16Khz32KBitRateMonoMp3)
    synthesizer = speechsdk.SpeechSynthesizer(speech_config=speech_config, audio_config=None)
    result = synthesizer.speak_text_async(text).get()
    if result.reason == speechsdk.ResultReason.SynthesizingAudioCompleted:
        return result.audio_data
    raise RuntimeError(f"–û—à–∏–±–∫–∞ —Å–∏–Ω—Ç–µ–∑–∞ —Ä–µ—á–∏: {result.cancellation_details.reason}")


# --- 6. –û—Å–Ω–æ–≤–Ω–æ–π —ç–Ω–¥–ø–æ–∏–Ω—Ç –¥–ª—è –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞ ---
@app.route('/api/ask-assistant', methods=['POST'])
def ask_assistant():
    init_data = request.headers.get('X-Telegram-Init-Data')
    if not init_data or not BOT_TOKEN:
        return jsonify({"error": "Auth data is missing or server is not configured"}), 401
    try:
        unquoted_init_data = unquote(init_data)
        data_check_string, hash_from_telegram = [], ''
        for item in sorted(unquoted_init_data.split('&')):
            key, value = item.split('=', 1)
            if key == 'hash': hash_from_telegram = value
            else: data_check_string.append(f"{key}={value}")
        data_check_string = "\n".join(data_check_string)
        secret_key = hmac.new("WebAppData".encode(), BOT_TOKEN.encode(), hashlib.sha256).digest()
        calculated_hash = hmac.new(secret_key, data_check_string.encode(), hashlib.sha256).hexdigest()
        if calculated_hash != hash_from_telegram:
            return jsonify({"error": "Invalid data signature"}), 403
    except Exception as e:
        logging.warning(f"–û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ Telegram initData: {e}")
        return jsonify({"error": "Could not validate Telegram credentials."}), 403
    
    try:
        if 'audio_file' not in request.files:
            return jsonify({"error": "No audio file part"}), 400
        
        history = json.loads(request.form.get('history_json', '[]'))
        audio_bytes = request.files['audio_file'].read()
        
        logging.info(f"‚úÖ [Assistant] –ü–æ–ª—É—á–µ–Ω –∞—É–¥–∏–æ—Ñ–∞–π–ª: {len(audio_bytes)} –±–∞–π—Ç.")

        recognized_text = recognize_speech_from_bytes(audio_bytes)
        answer_text = get_answer_from_llm(recognized_text, history)
        answer_audio_bytes = synthesize_speech_for_assistant(answer_text)
        audio_base64 = base64.b64encode(answer_audio_bytes).decode('utf-8')
        
        return jsonify({"userText": recognized_text, "assistantText": answer_text, "audioBase64": audio_base64})
    except ValueError as e:
        logging.warning(f"–û—à–∏–±–∫–∞ –¥–∞–Ω–Ω—ã—Ö –æ—Ç –∫–ª–∏–µ–Ω—Ç–∞ (400): {e}")
        return jsonify({"detail": str(e)}), 400
    except Exception as e:
        logging.error("–ù–µ–ø—Ä–µ–¥–≤–∏–¥–µ–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –≤ /api/ask-assistant", exc_info=True)
        return jsonify({"detail": "–ü—Ä–æ–∏–∑–æ—à–ª–∞ –Ω–µ–ø—Ä–µ–¥–≤–∏–¥–µ–Ω–Ω–∞—è –≤–Ω—É—Ç—Ä–µ–Ω–Ω—è—è –æ—à–∏–±–∫–∞ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞."}), 500

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000)